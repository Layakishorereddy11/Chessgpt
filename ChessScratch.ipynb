{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: install chess\n",
    "!pip install python-chess chess.engine\n",
    "\n",
    "!pip install python-chess~=0.26\n",
    "!pip install livelossplot==0.3.4\n",
    "!wget https://www.dropbox.com/sh/75gzfgu7qo94pvh/AACk_w5M94GTwwhSItCqsemoa/Stockfish%205/stockfish-5-linux.zip\n",
    "!unzip stockfish-5-linux.zip\n",
    "\n",
    "!chmod +x stockfish-5-linux/Linux/stockfish_14053109_x64\n",
    "\n",
    "import chess\n",
    "import chess.engine\n",
    "\n",
    "# Cell 1: Imports\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.modules.transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "# from chessformers.tokenizer import Tokenizer\n",
    "# from chessformers.configuration import get_configuration\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vocab_counter = set()\n",
    "\n",
    "    with open(f\"dataset/processed_kaggle2.txt\", \"w\", encoding=\"utf-8\") as outf:\n",
    "        with open(\"/content/dataset/all_with_filtered_anotations_since1998 copy (1).txt\", \"r\", encoding=\"utf-8\") as inpf:\n",
    "            for line in inpf:\n",
    "                try:\n",
    "                    ostr = line.split(\"###\")[1].strip()\n",
    "                    ostr = re.sub(\"W\\d+.\", \"\", ostr)\n",
    "                    ostr = re.sub(\"B\\d+.\", \"\", ostr)\n",
    "\n",
    "                    if len(ostr) > 0:\n",
    "                        if ostr[-1] != '\\n':\n",
    "                            ostr = ostr + '\\n'\n",
    "\n",
    "                        outf.write(ostr)\n",
    "\n",
    "                        for move in ostr.split(\" \"):\n",
    "                            move = move.replace(\"\\n\", \"\")\n",
    "\n",
    "                            if move != \"\":\n",
    "                                vocab_counter.add(move)\n",
    "                    else:\n",
    "                        a = 0\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        os.makedirs(\"vocabs\", exist_ok=True)\n",
    "\n",
    "        with open(f\"vocabs/kaggle2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for v in vocab_counter:\n",
    "                f.write(v + \"\\n\")\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "VOCAB_DIR = \"vocabs\"\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    pad_token_index: int = 0\n",
    "    bos_token_index: int = 1\n",
    "    eos_token_index: int = 2\n",
    "    unk_token_index: int = 3\n",
    "\n",
    "    pad_token: str = \"<pad>\"\n",
    "    bos_token: str = \"<bos>\"\n",
    "    eos_token: str = \"<eos>\"\n",
    "    unk_token: str = \"<unk>\"\n",
    "\n",
    "    def __init__(self, vocab_path: str = f\"{VOCAB_DIR}/kaggle2_vocab.txt\") -> None:\n",
    "        self.vocab_dict = {\n",
    "            self.pad_token: self.pad_token_index,\n",
    "            self.bos_token: self.bos_token_index,\n",
    "            self.eos_token: self.eos_token_index,\n",
    "            self.unk_token: self.unk_token_index,\n",
    "        }\n",
    "\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, token in enumerate(f):\n",
    "                self.vocab_dict[token.replace(\"\\n\", \"\")] = i + 4\n",
    "\n",
    "    def encode(self, token_str: str, add_bos_token=True):\n",
    "        encoded = []\n",
    "\n",
    "        if add_bos_token:\n",
    "            encoded.append(self.bos_token_index)\n",
    "\n",
    "        for token in token_str.split():\n",
    "            if token in self.vocab_dict:\n",
    "                encoded.append(self.vocab_dict[token])\n",
    "            else:\n",
    "                encoded.append(self.unk_token_index)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, token_ids: list):\n",
    "        decoded = []\n",
    "\n",
    "        for token_id in token_ids:\n",
    "            for token, index in self.vocab_dict.items():\n",
    "                if index == token_id:\n",
    "                    decoded.append(token)\n",
    "\n",
    "        return \" \".join(decoded)\n",
    "\n",
    "\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab_dict)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def generate_vocab(cls, dataset_path: str):\n",
    "        from pathlib import Path\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        vocab_counter = set()\n",
    "\n",
    "        for game in tqdm(Path(dataset_path).glob(\"*.txt\")):\n",
    "            game = game.read_text(encoding=\"utf-8\")\n",
    "            for move in game.split(\" \"):\n",
    "                move = move.replace(\"\\n\", \"\")\n",
    "\n",
    "                if move != \"\":\n",
    "                    vocab_counter.add(move)\n",
    "\n",
    "        os.makedirs(VOCAB_DIR, exist_ok=True)\n",
    "\n",
    "        with open(f\"{VOCAB_DIR}/kaggle2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for v in vocab_counter:\n",
    "                f.write(v + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Tokenizer.generate_vocab(\"dataset/kaggle2/\")\n",
    "    tokenizer = Tokenizer(\"/content/vocabs/kaggle2_vocab.txt\")\n",
    "    encoded = tokenizer.encode(\"d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 pepe Bb4+ Nc3 Ba5 Bf4 <eos>\")\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(encoded)\n",
    "    print(decoded)\n",
    "\n",
    "\n",
    "# Cell 2: PGNDataset Class\n",
    "class PGNDataset(Dataset):\n",
    "    def __init__(self, tokenizer: Tokenizer, path: str, n_positions=512):\n",
    "        self.n_positions = n_positions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.games = []\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.games.append(line)\n",
    "\n",
    "        print(\"Dataset read.\")\n",
    "\n",
    "    def __pad(self, sample: list):\n",
    "        while len(sample) < self.n_positions:\n",
    "            sample.append(self.tokenizer.pad_token_index)\n",
    "        return sample[:self.n_positions]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.games)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        game = self.games[i]\n",
    "        encoded = self.tokenizer.encode(game, add_bos_token=True)\n",
    "\n",
    "        if len(encoded) < self.n_positions:\n",
    "            encoded.append(self.tokenizer.eos_token_index)\n",
    "\n",
    "        data = self.__pad(encoded)\n",
    "        return torch.tensor(data)\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "\n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(\n",
    "            0, max_len, dtype=torch.float).view(-1, 1)  # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float(\n",
    "        ) * (-math.log(10000.0)) / dim_model)  # 1000^(2i/dim_model)\n",
    "\n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "\n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "\n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: Tokenizer,\n",
    "        num_tokens: int,\n",
    "        dim_model: int,\n",
    "        num_heads: int,\n",
    "        d_hid: int,\n",
    "        num_layers: int,\n",
    "        dropout_p: float,\n",
    "        n_positions: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "        self.n_positions = n_positions\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=n_positions\n",
    "        )\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_tokens, dim_model, padding_idx=self.tokenizer.pad_token_index)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            dim_model,\n",
    "            num_heads,\n",
    "            d_hid,\n",
    "            dropout_p,\n",
    "            batch_first=False,\n",
    "            activation=F.gelu,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            encoder_layers, num_layers)\n",
    "\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_pad_mask=None) -> torch.Tensor:\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        transformer_out = self.transformer_encoder(\n",
    "            src,\n",
    "            src_mask,\n",
    "            src_pad_mask,\n",
    "        )\n",
    "\n",
    "        out = self.out(transformer_out)\n",
    "\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "    def get_src_mask(self, sz) -> torch.Tensor:\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def get_pad_mask(self, matrix: torch.Tensor, pad_token: int) -> torch.Tensor:\n",
    "        return (matrix == pad_token).t()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        input_string: str = \"<bos>\",\n",
    "        max_length=80,\n",
    "        stop_at_next_move=False,\n",
    "        temperature=0.5\n",
    "    ) -> str:\n",
    "        import chess\n",
    "\n",
    "        board = chess.Board()\n",
    "        self.eval()\n",
    "\n",
    "        input_sequence = self.tokenizer.encode(\n",
    "            input_string, add_bos_token=False)\n",
    "\n",
    "        for token in input_string.split(\" \")[1:]:\n",
    "            board.push_san(token)\n",
    "\n",
    "        if board.is_checkmate():\n",
    "            input_string += \" <eos>\"\n",
    "\n",
    "        y_input = torch.tensor(\n",
    "            [input_sequence], dtype=torch.long, device=\"cpu\").t()\n",
    "\n",
    "        if stop_at_next_move:\n",
    "            max_length = 1\n",
    "        else:\n",
    "            max_length -= len(input_sequence)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            y_size = y_input.size(0)\n",
    "            begin_loc = max(y_size - self.n_positions, 0)\n",
    "\n",
    "            if y_size > self.n_positions and begin_loc % 2 != 0:\n",
    "                # Let's help the model know what turn it is\n",
    "                begin_loc += 1\n",
    "\n",
    "            end_loc = min(begin_loc + self.n_positions, y_size)\n",
    "            input_ids = y_input[begin_loc:end_loc]\n",
    "\n",
    "            src_mask = self.get_src_mask(input_ids.size(0)).to(\"cpu\")\n",
    "            pad_mask = self.get_pad_mask(\n",
    "                input_ids, self.tokenizer.pad_token_index).to(\"cpu\")\n",
    "\n",
    "            pred = self.forward(input_ids, src_mask, pad_mask)\n",
    "\n",
    "            word_weights = pred[-1].squeeze().div(temperature).exp()\n",
    "            word_idx = torch.multinomial(word_weights, 10)\n",
    "\n",
    "            for wi in word_idx:\n",
    "                decoded = self.tokenizer.decode([wi])\n",
    "                try:\n",
    "                    board.parse_san(decoded)\n",
    "                    word_idx = wi\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if word_idx.ndim > 0:\n",
    "                # If the model doesn't know what to move, surrenders\n",
    "                next_item = torch.tensor([[self.tokenizer.eos_token_index]], device=\"cpu\")\n",
    "                y_input = torch.cat((y_input, next_item), dim=0)\n",
    "                break\n",
    "\n",
    "            next_item = torch.tensor([[word_idx]], device=\"cpu\")\n",
    "            board.push_san(self.tokenizer.decode([next_item]))\n",
    "\n",
    "            # Concatenate previous input with predicted best word\n",
    "            y_input = torch.cat((y_input, next_item), dim=0)\n",
    "\n",
    "            if board.is_checkmate():\n",
    "                # If it checkmates the opponent, return with <eos>\n",
    "                next_item = torch.tensor([[self.tokenizer.eos_token_index]], device=\"cpu\")\n",
    "                y_input = torch.cat((y_input, next_item), dim=0)\n",
    "                break\n",
    "\n",
    "            # Stop if model predicts end of sentence\n",
    "            if next_item.view(-1).item() == self.tokenizer.eos_token_index:\n",
    "                break\n",
    "\n",
    "        return self.tokenizer.decode(y_input.view(-1).tolist())\n",
    "\n",
    "# Configuration Constants\n",
    "n_positions = 80\n",
    "dim_model = 768\n",
    "d_hid = 3072\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "dropout_p = 0.1\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Chessformers trainer parser')\n",
    "\n",
    "    # Providing default values for arguments\n",
    "    parser.add_argument('--tokenizer', type=str, default=\"vocabs/kaggle2_vocab.txt\", help='location of the tokenizer file')\n",
    "    parser.add_argument('--dataset', type=str, default=\"dataset/processed_kaggle2.txt\", help='location of the dataset')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='training batch size')\n",
    "    parser.add_argument('--epochs', type=int, default=1, help='number of training epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.00025, help='learning rate')\n",
    "    parser.add_argument('--beta1', type=float, default=0.9, help='adam beta')\n",
    "    parser.add_argument('--save_dir', type=str, default='./model', help='save model directory')\n",
    "    parser.add_argument('--load_model', type=str, default=None, help='model to load and resume training')\n",
    "\n",
    "    # Use parse_known_args to handle unknown arguments in Jupyter Notebook\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, loss_fn, save_dir, learning_rate, num_epochs, adam_beta):\n",
    "        self.save_dir = save_dir\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.lr = learning_rate\n",
    "        self.loss_fn = loss_fn\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.lr, betas=(adam_beta, 0.999))\n",
    "\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        print(f'Selected device: {self.device}.')\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        train_loss = []\n",
    "        for local_batch in tqdm(self.train_loader):\n",
    "            X = local_batch.to(self.device).t().contiguous()\n",
    "            y_input = X[:-1]\n",
    "            y_expected = X[1:].reshape(-1)\n",
    "\n",
    "            src_mask = self.model.get_src_mask(y_input.size(0)).to(self.device)\n",
    "            pad_mask = self.model.get_pad_mask(\n",
    "                y_input, self.model.tokenizer.pad_token_index).to(self.device)\n",
    "\n",
    "            pred = self.model(y_input, src_mask, pad_mask)\n",
    "            loss = self.loss_fn(pred.view(-1, self.model.tokenizer.vocab_size()), y_expected)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss.append(loss.detach().cpu().numpy())\n",
    "        return np.mean(train_loss)\n",
    "\n",
    "    def test_epoch(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for local_batch in self.val_loader:\n",
    "                X = local_batch.to(self.device).t().contiguous()\n",
    "                y_input = X[:-1]\n",
    "                y_expected = X[1:].reshape(-1)\n",
    "\n",
    "                src_mask = self.model.get_src_mask(y_input.size(0)).to(self.device)\n",
    "                pad_mask = self.model.get_pad_mask(\n",
    "                    y_input, self.model.tokenizer.pad_token_index).to(self.device)\n",
    "\n",
    "                pred = self.model(y_input, src_mask, pad_mask)\n",
    "                loss = self.loss_fn(pred.view(-1, self.model.tokenizer.vocab_size()), y_expected)\n",
    "                total_loss += loss\n",
    "        return total_loss / len(self.val_loader)\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\n -------- EPOCH {epoch + 1}/{self.num_epochs} --------\")\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss = self.test_epoch()\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.save_dir, f\"model_epoch_{epoch + 1}.pth\"))\n",
    "\n",
    "        torch.save(self.model.state_dict(), os.path.join(self.save_dir, \"final_model.pth\"))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    tokenizer = Tokenizer(args.tokenizer)\n",
    "\n",
    "    # Prepare the data\n",
    "    dataset = PGNDataset(tokenizer, args.dataset, n_positions=n_positions)\n",
    "    train_len = int(0.8 * len(dataset))\n",
    "    train_data, val_data = random_split(dataset, [train_len, len(dataset) - train_len])\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # Define the model\n",
    "    model = Transformer(\n",
    "        tokenizer=tokenizer,\n",
    "        num_tokens=tokenizer.vocab_size(),\n",
    "        dim_model=dim_model,\n",
    "        d_hid=d_hid,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        dropout_p=dropout_p,\n",
    "        n_positions=n_positions\n",
    "    )\n",
    "\n",
    "    if args.load_model:\n",
    "        print(\"Loading pre-trained model...\")\n",
    "        model.load_state_dict(torch.load(args.load_model))\n",
    "\n",
    "    loss_fn = torch.nn.NLLLoss(ignore_index=tokenizer.pad_token_index)\n",
    "    trainer = Trainer(model, train_loader, val_loader, loss_fn, args.save_dir, args.lr, args.epochs, args.beta1)\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = _parse_args()\n",
    "    main(args)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Script used to play against the chessformers.\n",
    "Human plays as white.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "\n",
    "n_positions = 80\n",
    "dim_model = 768\n",
    "d_hid = 3072\n",
    "num_heads = 12\n",
    "num_layers = 12\n",
    "dropout_p = 0.1\n",
    "\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Chessformers inference parser')\n",
    "\n",
    "    parser.add_argument('--load_model', type=str, default=\"/content/chessformer_epoch_13.pth\",\n",
    "                        help='model to load and do inference')\n",
    "\n",
    "    parser.add_argument('--tokenizer', type=str, default=\"/content/kaggle2_vocab.txt\",\n",
    "                        help='location of the tokenizer file')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args) -> None:\n",
    "    tokenizer = Tokenizer(args.tokenizer)\n",
    "    model = Transformer(tokenizer,\n",
    "                        num_tokens=tokenizer.vocab_size(),\n",
    "                        dim_model=dim_model,\n",
    "                        d_hid=d_hid,\n",
    "                        num_heads=num_heads,\n",
    "                        num_layers=num_layers,\n",
    "                        dropout_p=dropout_p,\n",
    "                        n_positions=n_positions,\n",
    "                        )\n",
    "    # model.load_state_dict(torch.load(args.load_model))\n",
    "    model.load_state_dict(torch.load(args.load_model, map_location=torch.device('cpu')))\n",
    "\n",
    "    print(\n",
    "        \"===== CHESSFORMERS ENGINE =====\\n\"\n",
    "    + \"    Enter valid moves in PGN format.\\n\"\n",
    "    + \"    Enter \\\\b to undo a move.\\n\"\n",
    "    + \"    Enter \\\\m to show all moves\\n\"\n",
    "    )\n",
    "\n",
    "    input_string = \"<bos>\"\n",
    "    boards = [input_string]\n",
    "\n",
    "    while (len(input_string.split(\" \")) < n_positions\n",
    "           and input_string.split(\" \")[-1] != tokenizer.eos_token):\n",
    "        next_move = input(\"WHITE MOVE: \")\n",
    "\n",
    "        if next_move == \"\\\\m\":\n",
    "            print(input_string)\n",
    "            continue\n",
    "        elif next_move == \"\\\\b\":\n",
    "            if len(boards) > 1:\n",
    "                boards.pop()\n",
    "\n",
    "            input_string = boards[-1]\n",
    "            continue\n",
    "\n",
    "        prev_input_string = input_string\n",
    "        input_string += \" \" + next_move\n",
    "        print(input_string)\n",
    "        try:\n",
    "            input_string = model.predict(\n",
    "                input_string,\n",
    "                stop_at_next_move=True,\n",
    "                temperature=0.2,\n",
    "                )\n",
    "            boards.append(input_string)\n",
    "            print(\"BLACK MOVE:\", input_string.split(\" \")[-1])\n",
    "        except ValueError:\n",
    "            input_string = prev_input_string\n",
    "            print(\"ILLEGAL MOVE. Please, try again.\")\n",
    "        except Exception as e:\n",
    "            print(\"UNHANDLED EXCEPTION. Please, try again.\")\n",
    "\n",
    "    print(\"--- Final board ---\")\n",
    "    print(input_string)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = _parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
